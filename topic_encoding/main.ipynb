{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64029041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "sentence_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # dim=384\n",
    "fields = ['title', 'description', 'channel', 'date']  # etc.\n",
    "field_embeds = sentence_encoder.encode(fields, convert_to_tensor=True)  # shape: [num_fields, 384]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class MetadataFusion(nn.Module):\n",
    "    def __init__(self, hidden_dim=384, num_fields=4):\n",
    "        super().__init__()\n",
    "        config = BertConfig(\n",
    "            hidden_size=hidden_dim,\n",
    "            num_attention_heads=6,\n",
    "            num_hidden_layers=2,\n",
    "            intermediate_size=hidden_dim * 4,\n",
    "            max_position_embeddings=num_fields + 1,  # +1 for [CLS]\n",
    "            num_labels=1\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))  # [CLS] token\n",
    "        \n",
    "        # Positional embeddings to help identify which field is which\n",
    "        self.field_type_embed = nn.Embedding(num_fields, hidden_dim)\n",
    "        self.num_fields = num_fields\n",
    "\n",
    "    def forward(self, field_embeddings):\n",
    "        \"\"\"\n",
    "        field_embeddings: Tensor of shape [batch_size, num_fields, hidden_dim]\n",
    "        \"\"\"\n",
    "        batch_size = field_embeddings.size(0)\n",
    "        device = field_embeddings.device\n",
    "        \n",
    "        # Add learned field-type embeddings (field identity)\n",
    "        field_positions = torch.arange(self.num_fields, device=device)\n",
    "        field_type_bias = self.field_type_embed(field_positions)  # [num_fields, hidden_dim]\n",
    "        field_type_bias = field_type_bias.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, num_fields, hidden_dim]\n",
    "        enriched_fields = field_embeddings + field_type_bias\n",
    "        \n",
    "        # Prepend [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # shape: [batch_size, 1, hidden_dim]\n",
    "        tokens = torch.cat([cls_tokens, field_embeddings], dim=1)  # [batch, num_fields + 1, dim]\n",
    "\n",
    "        attention_mask = torch.ones(tokens.shape[:2], dtype=torch.long).to(tokens.device)\n",
    "        output = self.bert(inputs_embeds=tokens, attention_mask=attention_mask)\n",
    "        fused = output.last_hidden_state[:, 0]  # take [CLS] token\n",
    "        return fused  # shape: [batch_size, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badf5af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: batch of 32 videos, each with 5 fields\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m field_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n",
      "\u001b[1;32m      3\u001b[0m     sentence_encoder\u001b[38;5;241m.\u001b[39mencode([title, desc, channel, date, tags], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (title, desc, channel, date, tags) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbatch\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m ])  \u001b[38;5;66;03m# shape: [32, 5, 384]\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m fuser \u001b[38;5;241m=\u001b[39m MetadataFusion(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m384\u001b[39m, num_fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[1;32m      8\u001b[0m fused_repr \u001b[38;5;241m=\u001b[39m fuser(field_embeds\u001b[38;5;241m.\u001b[39mcuda())  \u001b[38;5;66;03m# [32, 384]\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Example: batch of 32 videos, each with 5 fields\n",
    "field_embeds = torch.stack([\n",
    "    sentence_encoder.encode([title, desc, channel, date, tags], convert_to_tensor=True)\n",
    "    for (title, desc, channel, date, tags) in batch\n",
    "])  # shape: [32, 5, 384]\n",
    "\n",
    "fuser = MetadataFusion(hidden_dim=384, num_fields=5).cuda()\n",
    "fused_repr = fuser(field_embeds.cuda())  # [32, 384]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
